{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q import_ipynb\n",
    "import import_ipynb\n",
    "\n",
    "import env_setup\n",
    "import datasets\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# float -> list\n",
    "def fitBins(value: int):\n",
    "\n",
    "    #       .125            ->      [1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    #       .250            ->      [0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    #       .375            ->      [0, 0, 1, 0, 0, 0, 0, 0]\n",
    "    #                   ..........\n",
    "    #       .750            ->      [0, 0, 0, 0, 0, 1, 0, 0]\n",
    "    #       .875            ->      [0, 0, 0, 0, 0, 0, 1, 0]\n",
    "    #      1.000            ->      [0, 0, 0, 0, 0, 0, 0, 1]\n",
    "\n",
    "    bins = np.array([0]*8)\n",
    "    for i in range(1, 9):\n",
    "        if value == i/8:\n",
    "            bins[i-1] = 1\n",
    "    return bins\n",
    "\n",
    "\n",
    "#                      time note  vol\n",
    "#                      row  col   bins\n",
    "# list(list(float)) -> list(list(list(int)))\n",
    "def performance2BinsMHE(perfomance: np.array):\n",
    "\n",
    "    #   (N, 88)         ------>         (N, 704)\n",
    "    #------------------------------------------------------\n",
    "    performance_bins = np.empty([perfomance.shape[0], perfomance.shape[1] * 8], dtype=int)\n",
    "    # print(performance_bins.shape)\n",
    "\n",
    "    for f, frame in enumerate(perfomance):\n",
    "        frame_values = []\n",
    "        for v, value in enumerate(frame):\n",
    "            # print(f'{value} -> {fitBins(value)}')\n",
    "            for i in fitBins(value):\n",
    "                frame_values.append(i)\n",
    "\n",
    "        performance_bins[f] = frame_values\n",
    "    return performance_bins\n",
    "\n",
    "# row bins\n",
    "def buildVocab(corpus):\n",
    "    vocab = np.unique(corpus, axis=0)\n",
    "    return vocab\n",
    "\n",
    "# fitBins(.375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "serial = pandas.read_pickle('Data/Aguas_De_Marco.pkl')\n",
    "\n",
    "perf = serial.iloc[:, 9:]\n",
    "\n",
    "_, velocities = datasets.performanceDecode(perf)\n",
    "\n",
    "corpus = performance2BinsMHE(velocities)\n",
    "print(corpus.shape)\n",
    "\n",
    "vocab = buildVocab(corpus)\n",
    "print(vocab.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ix_to_word = {ix:word for ix, word in enumerate(vocab)}\n",
    "word_to_ix = {tuple(word):ix for ix, word in enumerate(vocab)}\n",
    "print(word_to_ix)\n",
    "np.save('Embeddings/ix_to_word', ix_to_word)\n",
    "np.save('Embeddings/word_to_ix', word_to_ix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 8 #\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "# F32\n",
    "#\n",
    "\n",
    "# F1, F2,...\n",
    "\n",
    "# build a list of tuples.\n",
    "# Each tuple is ([ word_i-CONTEXT_SIZE, ..., word_i-1 ], target word)\n",
    "ngrams = [\n",
    "    (\n",
    "        [corpus[i - j - 1] for j in range(CONTEXT_SIZE)],\n",
    "        corpus[i]\n",
    "    )\n",
    "    for i in range(CONTEXT_SIZE, len(corpus))\n",
    "]\n",
    "# Print the first 3, just so you can see what they look like.\n",
    "# print(len(ngrams), ngrams[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    def plot_loss_update(i, n, mb, train_loss):\n",
    "        '''\n",
    "            Dynamically print the loss plot during the training/validation loop.\n",
    "            Expects epoch to start from 1.\n",
    "        '''\n",
    "\n",
    "        mb.names = ['Loss']\n",
    "        x = range(1, i+1)\n",
    "        y = train_loss\n",
    "        graphs = [[x,train_loss]]\n",
    "        x_margin = 0.2\n",
    "        y_margin = 0.05\n",
    "        x_bounds = [1-x_margin, n+x_margin]\n",
    "        y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "        mb.update_graph(graphs, x_bounds, y_bounds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "N_EPOCHS = 6\n",
    "\n",
    "mb = master_bar(range(1, N_EPOCHS+1))\n",
    "\n",
    "for epoch in mb:\n",
    "    total_loss = 0\n",
    "    for index in progress_bar(range(len(ngrams)), parent=mb):\n",
    "        context, target = ngrams[index]\n",
    "\n",
    "        # Step 1. Prepare the inputs to be passed to the model (i.e, turn the words\n",
    "        # into integer indices and wrap them in tensors)\n",
    "        context_idxs = torch.tensor([word_to_ix[tuple(w)] for w in context], dtype=torch.long).to(device)\n",
    "\n",
    "        # Step 2. Recall that torch *accumulates* gradients. Before passing in a\n",
    "        # new instance, you need to zero out the gradients from the old\n",
    "        # instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 3. Run the forward pass, getting log probabilities over next\n",
    "        # words\n",
    "        log_probs = model(context_idxs)\n",
    "\n",
    "        # Step 4. Compute your loss function. (Again, Torch wants the target\n",
    "        # word wrapped in a tensor)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_to_ix[tuple(target)]], dtype=torch.long).to(device))\n",
    "\n",
    "        # Step 5. Do the backward pass and update the gradient\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(loss.item())\n",
    "\n",
    "        loss_step = loss.item()\n",
    "\n",
    "        mb.child.comment = f'[Loss step {loss_step:.8f}]'\n",
    "\n",
    "        # Get the Python number from a 1-element Tensor by calling tensor.item()\n",
    "        total_loss += loss_step\n",
    "\n",
    "\n",
    "    losses.append(total_loss/len(ngrams))\n",
    "\n",
    "    mb.main_bar.comment = f'[Epoch {epoch} | Loss {total_loss:.8f}]'\n",
    "    plot_loss_update(epoch, N_EPOCHS, mb, losses)\n",
    "\n",
    "torch.save(model.embeddings, f'Embeddings/test_embeddings.pkl')\n",
    "\n",
    "print(model.embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To get the emb\n",
    "\n",
    "print(vocab[55])\n",
    "# To get the embedding of a particular word, e.g. \"beauty\"\n",
    "A = model.embeddings.weight[word_to_ix[tuple(vocab[55])]]\n",
    "B = model.embeddings.weight[word_to_ix[tuple(vocab[56])]]\n",
    "print(A)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
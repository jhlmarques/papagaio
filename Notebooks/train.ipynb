{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from env_setup.ipynb\n",
      "importing Jupyter notebook from datasets.ipynb\n",
      "importing Jupyter notebook from lstm_predictor.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "\n",
    "import env_setup\n",
    "import datasets\n",
    "import lstm_predictor\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from music21 import instrument as m21instrument\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from fastprogress.fastprogress import master_bar, progress_bar"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "''\n",
    "def train(model, trainloader, resolution, optimizer, loss_fn, batch_size=1, num_epochs=5):\n",
    "        writer = SummaryWriter()\n",
    "\n",
    "        print(\"[Train Starting]\")\n",
    "\n",
    "        s_time = time.time()\n",
    "\n",
    "        mb = master_bar(range(1, num_epochs+1))\n",
    "        # mb.names = ['Loss', 'Beat Comparisons']\n",
    "\n",
    "        # Beat comparison counter\n",
    "        n_comparisons = 0\n",
    "        comparison_index = 0\n",
    "\n",
    "        train_loss = []\n",
    "\n",
    "        #\n",
    "        #   EPOCH ITERATION\n",
    "        #\n",
    "        for epoch_index in mb:\n",
    "\n",
    "            count = 0\n",
    "            training_loss = 0.0\n",
    "\n",
    "            # Amount of comparison blocks in the DL\n",
    "            train_iterator = iter(trainloader)\n",
    "            n_comparisons = len(train_iterator)\n",
    "            # print(f'[Train Iterator Length] {n_comparisons}')\n",
    "\n",
    "            # print(f'Input data shape: {input_data.shape}')\n",
    "            # print(f'Target data shape: {target_data.shape}')\n",
    "            # print(f'Target data: {target_data}')\n",
    "\n",
    "            # Initialize hidden and cells\n",
    "            (hidden, cell) = model.init_hidden(batch_size)\n",
    "\n",
    "            #\n",
    "            #   BEAT COMPARISON ITERATION\n",
    "            #\n",
    "            for comparison_index in progress_bar(range(n_comparisons), parent=mb):\n",
    "\n",
    "                # Add to total comparisons\n",
    "                # comparison_index += prev_n_comparisons\n",
    "\n",
    "                # print(f'\\n[Comparison #{comparison_index + 1}]\\n')\n",
    "\n",
    "                sample = next(train_iterator) # (2, 16, 88)\n",
    "\n",
    "                # Generate predictions\n",
    "                input_seq = sample[0].to(model.device) # (1, 16, 88)\n",
    "                target_seq = sample[1].to(model.device) # (1, 16, 88)\n",
    "\n",
    "                # pprint.pprint(input_seq)\n",
    "                # pprint.pprint(target_seq)\n",
    "\n",
    "                # print(f'\\t[Input Seq Shape] {input_seq.shape}')\n",
    "                # pprint.pprint(input_seq)\n",
    "\n",
    "                output_seq, (hidden, cell) = model(input_seq, hidden, cell)\n",
    "\n",
    "                # print(output_seq)\n",
    "\n",
    "                # print('\\t[Output Seq Shape] ', output_seq.shape)\n",
    "                # pprint.pprint(output_seq)\n",
    "\n",
    "                # output_seq = self.fc(output_seq[:, -1, :])\n",
    "                # print('b', output_seq.shape)\n",
    "                # output_seq = self.sigmoid(output_seq)\n",
    "                # print('c', output_seq.shape)\n",
    "\n",
    "                # print(f'Output seq shape: {output_seq.shape}')\n",
    "                # print('Target seq shape:', target_seq.shape)\n",
    "\n",
    "                # Compute the loss and backpropag\n",
    "\n",
    "                loss_step = loss_fn(output_seq, torch.squeeze(target_seq))\n",
    "\n",
    "                # print(f'\\t[Loss step] {loss_step}')\n",
    "\n",
    "                hidden = hidden.detach()\n",
    "                cell = cell.detach()\n",
    "\n",
    "                count += 1\n",
    "\n",
    "                # Backpropagate and compute gradients\n",
    "                loss_step.backward()\n",
    "                # Update net weights\n",
    "                optimizer.step()\n",
    "                # Clear gradients from the iteration\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Update loss accumulator\n",
    "                training_loss += loss_step.item()\n",
    "\n",
    "                mb.child.comment = f'[Loss step {loss_step:.8f}]'\n",
    "\n",
    "            # Scale loss accumulator\n",
    "            # training_loss /= len(dataloader.dataset)\n",
    "            training_loss /= len(train_iterator)\n",
    "\n",
    "            writer.add_scalar(\"Loss/train\", training_loss, epoch_index)\n",
    "\n",
    "            train_loss.append(training_loss)\n",
    "            # model.plot_loss_update(epoch_index, num_epochs, mb, training_loss)\n",
    "            mb.main_bar.comment = f'[Epoch {epoch_index} | Loss {training_loss:.8f}]'\n",
    "            print(f'[Epoch {epoch_index} | Loss {training_loss:.8f}]')\n",
    "            model.plot_loss_update(epoch_index, num_epochs, mb, train_loss)\n",
    "\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "\n",
    "        print(f'[Finished training with Loss {training_loss:.8f} (took {time.time() - s_time} seconds)]')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "= MODELO = \n",
      "\n",
      "LSTM(88, 128, num_layers=4, batch_first=True, bidirectional=True)\n",
      "\n",
      "\n",
      "Linear(in_features=256, out_features=88, bias=False)\n",
      "\n",
      "==========================================\n",
      "Dataset length: 1 songs.\n",
      "[Train Starting]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | Loss 0.69312577]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "image/png": {
       "width": 380,
       "height": 252
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2 | Loss 0.69063325]\n",
      "[Epoch 3 | Loss 0.68812565]\n",
      "[Epoch 4 | Loss 0.68554423]\n",
      "[Epoch 5 | Loss 0.68281697]\n",
      "[Epoch 6 | Loss 0.67987393]\n",
      "[Epoch 7 | Loss 0.67665617]\n",
      "[Epoch 8 | Loss 0.67311099]\n",
      "[Epoch 9 | Loss 0.66914505]\n",
      "[Epoch 10 | Loss 0.66468255]\n",
      "[Epoch 11 | Loss 0.65959902]\n",
      "[Epoch 12 | Loss 0.65373674]\n",
      "[Epoch 13 | Loss 0.64694305]\n",
      "[Epoch 14 | Loss 0.63902747]\n",
      "[Epoch 15 | Loss 0.62974698]\n",
      "[Epoch 16 | Loss 0.61883846]\n",
      "[Epoch 17 | Loss 0.60598969]\n",
      "[Epoch 18 | Loss 0.59082895]\n",
      "[Epoch 19 | Loss 0.57296164]\n",
      "[Epoch 20 | Loss 0.55197643]\n",
      "[Epoch 21 | Loss 0.52749721]\n",
      "[Epoch 22 | Loss 0.49925788]\n",
      "[Epoch 23 | Loss 0.46730455]\n",
      "[Epoch 24 | Loss 0.43202339]\n",
      "[Epoch 25 | Loss 0.39422387]\n",
      "[Epoch 26 | Loss 0.35509014]\n",
      "[Epoch 27 | Loss 0.31609960]\n",
      "[Epoch 28 | Loss 0.27873178]\n",
      "[Epoch 29 | Loss 0.24422665]\n",
      "[Epoch 30 | Loss 0.21343002]\n",
      "[Epoch 31 | Loss 0.18669825]\n",
      "[Epoch 32 | Loss 0.16396845]\n",
      "[Epoch 33 | Loss 0.14490876]\n",
      "[Epoch 34 | Loss 0.12906191]\n",
      "[Epoch 35 | Loss 0.11592437]\n",
      "[Epoch 36 | Loss 0.10502341]\n",
      "[Epoch 37 | Loss 0.09594768]\n",
      "[Epoch 38 | Loss 0.08835253]\n",
      "[Epoch 39 | Loss 0.08195702]\n",
      "[Epoch 40 | Loss 0.07653539]\n",
      "[Epoch 41 | Loss 0.07190701]\n",
      "[Epoch 42 | Loss 0.06792807]\n",
      "[Epoch 43 | Loss 0.06448395]\n",
      "[Epoch 44 | Loss 0.06148298]\n",
      "[Epoch 45 | Loss 0.05885148]\n",
      "[Epoch 46 | Loss 0.05652918]\n",
      "[Epoch 47 | Loss 0.05446773]\n",
      "[Epoch 48 | Loss 0.05262932]\n",
      "[Epoch 49 | Loss 0.05098156]\n",
      "[Epoch 50 | Loss 0.04949770]\n",
      "[Epoch 51 | Loss 0.04815553]\n",
      "[Epoch 52 | Loss 0.04693649]\n",
      "[Epoch 53 | Loss 0.04582501]\n",
      "[Epoch 54 | Loss 0.04480795]\n",
      "[Epoch 55 | Loss 0.04387413]\n",
      "[Epoch 56 | Loss 0.04301406]\n",
      "[Epoch 57 | Loss 0.04221957]\n",
      "[Epoch 58 | Loss 0.04148365]\n",
      "[Epoch 59 | Loss 0.04080022]\n",
      "[Epoch 60 | Loss 0.04016403]\n",
      "[Epoch 61 | Loss 0.03957049]\n",
      "[Epoch 62 | Loss 0.03901558]\n",
      "[Epoch 63 | Loss 0.03849573]\n",
      "[Epoch 64 | Loss 0.03800781]\n",
      "[Epoch 65 | Loss 0.03754906]\n",
      "[Epoch 66 | Loss 0.03711703]\n",
      "[Epoch 67 | Loss 0.03670952]\n",
      "[Epoch 68 | Loss 0.03632457]\n",
      "[Epoch 69 | Loss 0.03596043]\n",
      "[Epoch 70 | Loss 0.03561551]\n",
      "[Epoch 71 | Loss 0.03528837]\n",
      "[Epoch 72 | Loss 0.03497771]\n",
      "[Epoch 73 | Loss 0.03468245]\n",
      "[Epoch 74 | Loss 0.03440147]\n",
      "[Epoch 75 | Loss 0.03413370]\n",
      "[Epoch 76 | Loss 0.03387842]\n",
      "[Epoch 77 | Loss 0.03363478]\n",
      "[Epoch 78 | Loss 0.03340201]\n",
      "[Epoch 79 | Loss 0.03317944]\n",
      "[Epoch 80 | Loss 0.03296642]\n",
      "[Epoch 81 | Loss 0.03276241]\n",
      "[Epoch 82 | Loss 0.03256684]\n",
      "[Epoch 83 | Loss 0.03237923]\n",
      "[Epoch 84 | Loss 0.03219914]\n",
      "[Epoch 85 | Loss 0.03202612]\n",
      "[Epoch 86 | Loss 0.03185979]\n",
      "[Epoch 87 | Loss 0.03169978]\n",
      "[Epoch 88 | Loss 0.03154576]\n",
      "[Epoch 89 | Loss 0.03139741]\n",
      "[Epoch 90 | Loss 0.03125443]\n",
      "[Epoch 91 | Loss 0.03111654]\n",
      "[Epoch 92 | Loss 0.03098350]\n",
      "[Epoch 93 | Loss 0.03085505]\n",
      "[Epoch 94 | Loss 0.03073097]\n",
      "[Epoch 95 | Loss 0.03061106]\n",
      "[Epoch 96 | Loss 0.03049510]\n",
      "[Epoch 97 | Loss 0.03038292]\n",
      "[Epoch 98 | Loss 0.03027435]\n",
      "[Epoch 99 | Loss 0.03016920]\n",
      "[Epoch 100 | Loss 0.03006734]\n",
      "[Epoch 101 | Loss 0.02996861]\n",
      "[Epoch 102 | Loss 0.02987288]\n",
      "[Epoch 103 | Loss 0.02978001]\n",
      "[Epoch 104 | Loss 0.02968989]\n",
      "[Epoch 105 | Loss 0.02960239]\n",
      "[Epoch 106 | Loss 0.02951741]\n",
      "[Epoch 107 | Loss 0.02943484]\n",
      "[Epoch 108 | Loss 0.02935459]\n",
      "[Epoch 109 | Loss 0.02927656]\n",
      "[Epoch 110 | Loss 0.02920064]\n",
      "[Epoch 111 | Loss 0.02912679]\n",
      "[Epoch 112 | Loss 0.02905491]\n",
      "[Epoch 113 | Loss 0.02898492]\n",
      "[Epoch 114 | Loss 0.02891676]\n",
      "[Epoch 115 | Loss 0.02885035]\n",
      "[Epoch 116 | Loss 0.02878563]\n",
      "[Epoch 117 | Loss 0.02872253]\n",
      "[Epoch 118 | Loss 0.02866100]\n",
      "[Epoch 119 | Loss 0.02860099]\n",
      "[Epoch 120 | Loss 0.02854245]\n",
      "[Epoch 121 | Loss 0.02848530]\n",
      "[Epoch 122 | Loss 0.02842951]\n",
      "[Epoch 123 | Loss 0.02837503]\n",
      "[Epoch 124 | Loss 0.02832183]\n",
      "[Epoch 125 | Loss 0.02826984]\n",
      "[Epoch 126 | Loss 0.02821905]\n",
      "[Epoch 127 | Loss 0.02816939]\n",
      "[Epoch 128 | Loss 0.02812077]\n",
      "[Epoch 129 | Loss 0.02807321]\n",
      "[Epoch 130 | Loss 0.02802677]\n",
      "[Epoch 131 | Loss 0.02798133]\n",
      "[Epoch 132 | Loss 0.02793686]\n",
      "[Epoch 133 | Loss 0.02789334]\n",
      "[Epoch 134 | Loss 0.02785074]\n",
      "[Epoch 135 | Loss 0.02780902]\n",
      "[Epoch 136 | Loss 0.02776816]\n",
      "[Epoch 137 | Loss 0.02772814]\n",
      "[Epoch 138 | Loss 0.02768893]\n",
      "[Epoch 139 | Loss 0.02765050]\n",
      "[Epoch 140 | Loss 0.02761283]\n",
      "[Epoch 141 | Loss 0.02757591]\n",
      "[Epoch 142 | Loss 0.02753977]\n",
      "[Epoch 143 | Loss 0.02750425]\n",
      "[Epoch 144 | Loss 0.02746942]\n",
      "[Epoch 145 | Loss 0.02743524]\n",
      "[Epoch 146 | Loss 0.02740172]\n",
      "[Epoch 147 | Loss 0.02736882]\n",
      "[Epoch 148 | Loss 0.02733653]\n",
      "[Epoch 149 | Loss 0.02730483]\n",
      "[Epoch 150 | Loss 0.02727371]\n",
      "[Finished training with Loss 0.02727371 (took 755.846647977829 seconds)]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# Data Settings\n",
    "path = 'C_major_scale'\n",
    "resolution = 16\n",
    "keyboard_size = 88\n",
    "\n",
    "# Model Parameters\n",
    "input_size = 88\n",
    "num_layers = 4\n",
    "hidden_size = 128\n",
    "\n",
    "\n",
    "# embedding = torch.load('Embeddings/test_embeddings.pkl')\n",
    "\n",
    "# model = lstm_predictor.PerformanceLSTM(device, hidden_size=36)\n",
    "model = lstm_predictor.PerformanceLSTM(\n",
    "                                       # embedding,\n",
    "                                       input_size,\n",
    "                                       hidden_size,\n",
    "                                       num_layers,\n",
    "                                       device)\n",
    "\n",
    "\n",
    "# test\n",
    "pianoDataset = datasets.InstrumentDataset(path, 'Piano', keyboard_size)\n",
    "pianoDataLoader = datasets.DataLoader(pianoDataset, batch_size=1, shuffle=True)\n",
    "n_songs = len(pianoDataset)\n",
    "print(f'Dataset length: {n_songs} songs.')\n",
    "\n",
    "# print(pianoDataset[0])\n",
    "\n",
    "\n",
    "for i, song in enumerate(pianoDataLoader):\n",
    "\n",
    "    # Clause guard for empty instrument in song\n",
    "    if len(song) == 0:\n",
    "        continue\n",
    "\n",
    "    mhe, velocities = song\n",
    "    mhe = mhe[0]\n",
    "    velocities = velocities[0]\n",
    "\n",
    "    # print(type(song), '\\n' ,len(song), song)\n",
    "    # mhe_training_samples = datasets.TrainingSamples(mhe, resolution)\n",
    "    training_samples = datasets.TrainingSamples(velocities, resolution)\n",
    "\n",
    "    # DL depth = 1\n",
    "    training_dl1 = datasets.DataLoader(training_samples, batch_size=1, shuffle=False)\n",
    "\n",
    "    # DL depth = 4\n",
    "    # training_dl4 = datasets.DataLoader(training_samples, batch_size=4, shuffle=False)\n",
    "\n",
    "    # print(len(training_dl1))\n",
    "    # print(len(training_dl4))\n",
    "\n",
    "    # for sample in training_dl1:\n",
    "    #     print(sample[1].shape)\n",
    "\n",
    "\n",
    "    lr = 3e-3 # Learning Rate\n",
    "    # adam_optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    sgd_optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCELoss() # Binary Cross Entropy\n",
    "    n_epochs = 150 # Train 3 times in each song\n",
    "\n",
    "    train(model, training_dl1, resolution, sgd_optimizer, loss_fn, batch_size=1, num_epochs=n_epochs)\n",
    "\n",
    "    torch.save(model, f'Models/pop1_{n_songs}_songs_{n_epochs}_epochs.pkl')\n",
    "\n",
    "    # (200, 16, 88) -> 200x(1, 16, 88)\n",
    "\n",
    "    # for sample in training_dl11:\n",
    "    #     model.train(training_dl1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}